{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"clearfix\" style=\"padding: 10px; padding-left: 0px\">\n",
    "<a href=\"http://bombora.com\"><img src=\"https://app.box.com/shared/static/e0j9v1xjmubit0inthhgv3llwnoansjp.png\" width=\"200px\" class=\"pull-right\" style=\"display: inline-block; margin: 5px; vertical-align: middle;\"></a>\n",
    "<h1> Bombora Data Science: <br> *Interview Exam* </h1>\n",
    "</div>\n",
    "\n",
    "<img width=\"200px\" src=\"https://app.box.com/shared/static/15slg1mvjd1zldbg3xkj9picjkmhzpa5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Welcome\n",
    "\n",
    "Welcome! This notebook contains interview exam questions referenced in the *Instructions* section in the `README.md`—please read that first, *before* attempting to answer questions here.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\" style=\"margin: 10px\">\n",
    "<p style=\"font-weight:bold\">ADVICE</p>\n",
    "<p>*Do not* read these questions, and panic, *before* reading the instructions in `README.md`.</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n",
    "<p style=\"font-weight:bold\">WARNING</p>\n",
    "\n",
    "<p>If using <a href=\"https://try.jupyter.org\">try.jupyter.org</a> do not rely on the server for anything you want to last - your server will be <span style=\"font-weight:bold\">deleted after 10 minutes of inactivity</span>. Save often and rember download notebook when you step away (you can always re-upload and start again)!</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "## Have fun!\n",
    "\n",
    "Regardless of outcome, getting to know you is important. Give it your best shot and we'll look forward to following up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Algo + Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 1.1: Fibionacci\n",
    "![fib image](https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Fibonacci_spiral_34.svg/200px-Fibonacci_spiral_34.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.1\n",
    "Given $n$ where $n \\in \\mathbb{N}$ (i.e., $n$ is an integer and $n > 0$), write a function `fibonacci(n)` that computes the Fibonacci number $F_n$, where $F_n$ is defined by the recurrence relation:\n",
    "\n",
    "$$ F_n = F_{n-1} + F_{n-2}$$\n",
    "\n",
    "with initial conditions of:\n",
    "\n",
    "$$ F_1 = 1,  F_2 = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci(n):\n",
    "    '''computes the fibonacci number 𝐹𝑛 defined by the recurrence \n",
    "    relation - 𝐹𝑛=𝐹𝑛−1+𝐹𝑛−2 with 𝐹1=1,𝐹2=1'''\n",
    "    \n",
    "    #check validity of input\n",
    "    if (not isinstance(n, int)) or (n<=0):\n",
    "        print(\"The input is invalid! 'n' should an integer that is greater than 0.\") \n",
    "    else:\n",
    "        #initial conditions\n",
    "        F1, F2 = 1, 1\n",
    "        for i in range(1,n):\n",
    "            F1, F2 = F2, F1 + F2\n",
    "        return F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first element i.e. F1 =  1\n",
      "The second element i.e. F2 =  1\n",
      "The fourth element i.e. F4 =  3\n",
      "The seventh element i.e. F7 =  13\n",
      "The tenth element i.e. F10 =  55\n"
     ]
    }
   ],
   "source": [
    "print(\"The first element i.e. F1 = \",fibonacci(1))\n",
    "print(\"The second element i.e. F2 = \",fibonacci(2))\n",
    "print(\"The fourth element i.e. F4 = \",fibonacci(4))\n",
    "print(\"The seventh element i.e. F7 = \",fibonacci(7))\n",
    "print(\"The tenth element i.e. F10 = \",fibonacci(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.2\n",
    "What's the complexity of your implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "Time Complexity: O(n)\n",
    "\n",
    "Space Complexity: O(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.3\n",
    "Consider an alternative implementation to compute Fibonacci number $F_n$ and write a new function, `fibonacci2(n)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(F, M):\n",
    "    '''function for computing the multiplication\n",
    "    of the matrix'''\n",
    "    x = (F[0][0] * M[0][0] + \n",
    "         F[0][1] * M[1][0]) \n",
    "    y = (F[0][0] * M[0][1] + \n",
    "         F[0][1] * M[1][1]) \n",
    "    z = (F[1][0] * M[0][0] + \n",
    "         F[1][1] * M[1][0]) \n",
    "    w = (F[1][0] * M[0][1] + \n",
    "         F[1][1] * M[1][1]) \n",
    "    F[0][0] = x \n",
    "    F[0][1] = y \n",
    "    F[1][0] = z \n",
    "    F[1][1] = w \n",
    "\n",
    "def power(F, n):\n",
    "    '''Optimized version for computing the power \n",
    "    using recursive multiplication'''\n",
    "    if( n == 0 or n == 1): \n",
    "        return; \n",
    "    M = [[1, 1], \n",
    "         [1, 0]]; \n",
    "    power(F, n // 2) \n",
    "    multiply(F, F) \n",
    "    if (n % 2 != 0): \n",
    "        multiply(F, M) \n",
    "\n",
    "def fibonacci2(n):\n",
    "    '''optimized method that computes the fibonacci number 𝐹𝑛 \n",
    "    defined by the recurrence relation - 𝐹𝑛=𝐹𝑛−1+𝐹𝑛−2 with 𝐹1=1,𝐹2=1'''\n",
    "    \n",
    "    #check validity of input\n",
    "    if (not isinstance(n, int)) or (n<=0):\n",
    "        print(\"The input is invalid! 'n' should an integer that is greater than 0.\")  \n",
    "    else:\n",
    "        F = [[1, 1], \n",
    "             [1, 0]]        \n",
    "        power(F, n - 1) \n",
    "\n",
    "        return F[0][0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first element i.e. F1 =  1\n",
      "The second element i.e. F2 =  1\n",
      "The fourth element i.e. F4 =  3\n",
      "The seventh element i.e. F7 =  13\n",
      "The tenth element i.e. F10 =  55\n"
     ]
    }
   ],
   "source": [
    "print(\"The first element i.e. F1 = \",fibonacci2(1))\n",
    "print(\"The second element i.e. F2 = \",fibonacci2(2))\n",
    "print(\"The fourth element i.e. F4 = \",fibonacci2(4))\n",
    "print(\"The seventh element i.e. F7 = \",fibonacci2(7))\n",
    "print(\"The tenth element i.e. F10 = \",fibonacci2(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.4\n",
    "What's the complexity of your implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "Time Complexity: O(logn)\n",
    "\n",
    "Space Complexity: O(logn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.5\n",
    "What are some examples of optimizations that could improve computational performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "The methods implemented above are quick in comparison to the general recursion approach used to compute the sequence. While the code might be simple to understand using the recursive approach, the computation complexity is much higher in that case. The first approach that I have used stores only the last two numbers and computes the fibonacci sequence by saving computation as well as memory. The second one improves upon the time complexity by using recursion call to optimize the calculation of power and the matrix multipliction but it takes additional memory (O(logn)) due to stack size of the funcation call. To further improve the computational performance, we could implement the Binet's formula (utilizes the golden ratio) that is used for computing the Fibonacci sequence. Using the formula for computation, we would only have the time complexity as O(1) and space complexity of O(1). However, it only provides approximate results and therefore might not yield accurate results in case of larger numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prob + Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.6: Pick your prize\n",
    "<img src=https://miro.medium.com/max/1100/1*m5b3O9sE68UCXjLw5oxy2g.png width=\"480\">\n",
    "\n",
    "A prize is placed at random behind one of three doors and you are asked to pick a door. To be concrete, say you always pick door 1. Now the game host chooses one of door 2 or 3, opens it and shows you that it is empty. They then give you the option to keep your picked door or switch to the unopened door. Should you stay or switch if you want to maximize your probability of winning the prize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "To maximize the probability of winning the prize, you should switch the door. After choosing the first door, there are three scenarios - \n",
    "\n",
    "\n",
    "Case 1: The prize is behind door 1 (probability = 1/3)   (Switching loses)\n",
    "\n",
    "Case 2: The prize is behind door 2 (probability = 1/3)   (Switching wins)\n",
    "\n",
    "Case 3: The prize is behind door 3 (probability = 1/3)   (Switching wins)\n",
    "\n",
    "\n",
    "Since we always pick up door 1, the game host will either open door 2 or door 3. After the game host opens one door (door 2 or door 3), the prize is either at door 1 or the door that wasn't opened. Let's consider the probability of winning if we switch in every case. In case 1, since the prize is behind door 1, if we switch we lose. In case 2, the prize is behind door 2, so the host will always open door 3. So, if we switch, we will be at door 2 and we will win the prize. In case 3, since the prize is behind door 3, the host will open door 2, and therefore if we switch, we will go to door 3 and we would win. Therefore, we win in two out of three cases. Hence, \n",
    "\n",
    "\n",
    "Probability of winning given we switch = 2/3\n",
    "\n",
    "\n",
    "So, we should switch to maximize our probability of winning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Conceptual ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 3.1 Why study gradient boosting or neural networks?\n",
    "\n",
    "Consider a regression setting where $X \\in \\mathbb{R}^p$ and $Y \\in \\mathbb{R}$. The goal is to come up with a function $f(X): \\mathbb{R}^p \\rightarrow \\mathbb{R}$ that minimizes the squared-error loss $(Y - f(X))^2$. Since X, Y are random variables, we seek to minimize the expectation of the squared error loss as follows\n",
    "\\begin{equation}\n",
    "EPE(f) = \\mathbb{E}\\left[(Y-f(X)^2\\right]\n",
    "\\end{equation}\n",
    "where EPE stands for expected prediction error. One can show that minimizing the expected prediction error leads to the following _regression function_\n",
    "\\begin{equation}\n",
    "f(x) = \\mathbb{E}\\left[Y|X=x\\right]\n",
    "\\end{equation}\n",
    "\n",
    "The goal of any method is to approximate the regression function above, which we denote as $\\hat{f}(x)$. For example, linear regression explicitly assumes that the regression function is approximately linear in its arguments, i.e. $\\hat{f}(x) = x^T\\beta$ while a neural network provides a nonlinear approximation of the regression function. \n",
    "\n",
    "The simplest of all these methods is [k-nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). Given $x$ and some neighbourhood of $k$ points $N_k(x)$, $\\hat{f}(x)$ is simply the average of all $y_i|x_i \\in N_k(x)$.  Let $N$ denote the training sample size. Under mild regularity conditions on the joint probability distribution $Pr(X, Y)$, one can show that as $N \\rightarrow \\infty$, $k \\rightarrow \\infty$ such that $k/N \\rightarrow 0$, then $\\hat{f}(x) \\rightarrow f(x)$ where $\\rightarrow$ means approaches or goes to. In other words, the k-nearest neighbors algorithm converges to the ideal solution as both the training sample size and number of neighbors increase to infinity.\n",
    "\n",
    "Now given this _universal approximator_, why look any further and research other methods? Please share your thoughts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "\n",
    "While k-nearest neighbors could be useful in many applications, there are many instances in which the model might incur more cost or computation, and we might prefer other classes of methods that would be meaningful in those situations. k-nearest neighbors algorithm might converge to the ideal solution as both the training sample size and the number of neighbors increases to infinity but in practice in a lot of applications, it is really difficult to get infinite or large training data. Some applications have a large cost in acquiring new data, in which case other methods might outperform k-nearest neighbors easily. Additionally, there is a large computation cost during runtime if the sample size is large. This is due to the fact that k-NN is very computationally expensive in real-time execution (since it explores the neighborhood to find the most similar groups) for tasks such as regression and classification. There are many applications that have constraints regarding fast inference. Moreover, we need to properly scale the various attributes to provide fair treatment among features. The model is also really sensitive to colinearity and outliers and therefore we need to take additional steps to identify and treat outliers or anomalies. Another challenge while using k-nearest neighbors is that we need to have a clear understanding of the input domain. The domain expertise is needed to determine the number of neighbors (k) and to decide and interpret the groups. Model interpretation becomes difficult in cases where we cannot use distance or similarity to distinguish between various groups. In many instances where we have a large number of features, the groups might not be very similar as the average distance can increase between the groups. It would be best to use a method dependent on the constraints, costs, expected value of the problem and hence we need to research other methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
